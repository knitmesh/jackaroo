

## 目录

[TOC]

云平台缓存场景概述:
-------------------------

openstack中可以使用cache层来缓存数据，Liberty版本主要有以下几种场景：

* 存储函数执行结果 keystone heat nova等项目把一些固定的属性和查询请求的结果放到cache里面，加速访问。
* 存储keystone token token创建完成之后，不需要修改，但会有大量的读操作，适合放到cache中
* 存储keystonemiddleware token 为neutron，cinder，nova等各个组件缓存从keystone获得的token。
* 存储业务层用户session, api接口缓存 主要是django支持使用


#### 1. 存储函数执行结果
   优势在于，很多查询函数的结果是固定的，但是又比较常用，查询一次之后，按照key-value存到cache中，再次查询时不需要访问数据库，直接从内存缓存中根据key将结果取出，可以提高处理速度。或者还有一些查询请求，查询的参数千变万化，但是结果只有固定的几类，这种更适合使用cache加速。


Liberty 版本openstack主要是nova和keystone这方面用的比较多，来看nova中的一个例子:
```
def get_instance_availability_zone(context, instance):
    """Return availability zone of specified instance."""
    host = instance.get('host')
    if not host:
        # 如果虚拟机还没有被分配到主机上，就把创建虚拟机时指定的zone信息取出返回
        az = instance.get('availability_zone')
        return az

    #虚拟机所在的zone取决于虚拟机所在物理机的归属的zone，所以可以生成一个
    #'azcache-主机名'样式的字符串作为cache key
    cache_key = _make_cache_key(host)
    #获取一个连接cache的client
    cache = _get_cache()
    #尝试取出这个key在cache中的值，作为zone信息，可能为空，也可能直接是结果
    az = cache.get(cache_key)
    #取出实例对象中的availability_zone属性
    az_inst = instance.get('availability_zone')
    if az_inst is not None and az != az_inst:
        #对象属性中有zone信息，且和cache取到的zone信息不一致，那么需要重新获取zone信息
        #并且更新到cache中
        az = None
    if not az:
        #如果cache中没有取到zone信息，或者是在上一步中zone信息被清空，重新获取zone信息
        elevated = context.elevated()
        az = get_host_availability_zone(elevated, host)
        #更新cache
        cache.set(cache_key, az)
    #返回zone信息
    return az 
```

这样除了第一次查询到这台物理机上的虚拟机外，查询这台物理机上的任何虚拟机所属的zone，再也不需要访问数据库，这样极大地减少了对数据库的请求数量，提高了响应速度。
这里支持的cache后端包括memcached,redis,mongondb或者是python的dict.目前主流openstack发行版推荐的选项是memcached，简单稳定，性能和功能够用。


#### 2. 存储keystone token
keystone中除了Fernet格式的token外，其他格式的token都需要由keystone存储起来，存储支持以下几种driver:

* sql 将token存放在数据库中，使用这种方法需要定期清理过期token，防止token表太大影响性能。
* memcache 将token存放到memcache中，token本身创建之后不会被修改，只会被读，所以很适合放到cache中，加速访问，放到cache中也不需要写脚本定期清理数据库中的过期token。
* memcache_pool 在memcache的基础上，实现了memcache的连接池，可以在线程之间复用连接。

一个常见的memcache token driver的配置可能是这样的:
```
[token]
caching = true
provider = keystone.token.providers.uuid.Provider
driver = keystone.token.persistence.backends.memcache.Token
[memcache]
servers = controller-1:11211,controller-2:11211,controller-3:11211
```


memcache driver的缺点在于，memcache本身是分布式的设计，但是并不是高可用的，如果controller-1上的的cache服务被重启，这个节点上的所有token都会丢失掉，会带来一些错误。

比这更糟糕的是，如果controller1网络不可达或者宕机，那么我们会观察到几乎每个openstack api请求都会有3s以上的卡顿。 

这是因为openstack默认使用python-memcached访问memcache，它提供的操作keystone的client继承自Thread.local类，和构建它的线程绑定。openstack服务启动后，会启动一定数量的子进程，每个http request到达，会有一个子进程接收，孵化一个线程去处理这个请求。  

如果用到memcache，线程会调用python-memcached构建一个client类，通过这个client的实例对memcache进行操作。  

如果访问到网络不可达的memcache节点，卡住，操作超时，将这个节点标识为30秒内不可用，在这个线程内，不会再因此而卡住，但是这个http请求结束之后，下一个http请求过来，重新孵化的线程会reinit这个client，新的client丢失了旧的client的状态，还是可能会访问到卡住的memcache节点上。

社区之所以要做memcache_pool，就是为了解决这个问题，将client统一由pool管理起来，memcache节点的状态，也由pool管理起来，这样每个子进程里只会卡第一次，所以强烈推荐使用memcache_pool驱动而不是memcache。社区将memcache_pool的代码从keystone复制到oslo_cache项目中，希望所有使用memcache的项目都通过它的memcachepool去访问，避免这个问题。其中，nova在M版本支持，heat在L版本支持。

#### 3. 存储keystonemiddleware token
我们请求任何openstack服务时，该服务都要校验请求中提供的token是否合理，这部分代码显然不是每个项目都自己实现一遍，它在keystonemiddleware项目实现，并作为filter配置在各个项目的api-paste.ini文件中,如下所示:
```
[filter:authtoken]
paste.filter_factory = keystonemiddleware.auth_token:filter_factory
```
当请求到达服务时，由keystonemiddleware访问keystone来查询请求携带的token是否合法，通常我们一个token会使用很多次，所以keystonemiddleware建议使用memcache缓存，把从keystone取到的token缓存一段时间，默认是300秒，以减少对keystone的压力，提高性能。kolla项目中nova keystonemiddleware配置示例如下:
```
[keystone_authtoken]
[keystone_authtoken]
auth_uri = {{ internal_protocol }}://{{ kolla_internal_fqdn }}:{{ keystone_public_port }}
auth_url = {{ admin_protocol }}://{{ kolla_internal_fqdn }}:{{ keystone_admin_port }}
auth_plugin = password
project_domain_id = default
user_domain_id = default
project_name = service
username = {{ nova_keystone_user }}
password = {{ nova_keystone_password }}

memcache_security_strategy = ENCRYPT
memcache_secret_key = {{ memcache_secret_key }}
memcached_servers = {% for host in groups['memcached'] %}{{ hostvars[host]['ansible_' + hostvars[host]['api_interface']]['ipv4']['address'] }}:{{ memcached_port }}{% if not loop.last %},{% endif %}{% endfor %}
```
如果不配置其中的memcached_servers的的话，每个服务每个子进程都会创建一个字典作为cache存放从keystone获得的token，但是字典内容都是相似的，如果使用统一的memcache，就不会有这方面的问题。现在master版本keystonemiddleware认为这是不合适的，会导致不一致的结果和较高的内存占用，计划在N或者O版本移除这个特性。

#### 4. 存储portal用户session, api接口缓存
django可以指定session存储后端为memcache, 另外在openstack client层, 我们也做了代理, 用于缓存加速


平台目前状况:
-------------------------
#### 1. 存储函数执行结果

通过查看配置发现目前只有nova使用了函数结果缓存, nova主要使用memcache存储函数执行结果, 例如az, metadata
openstack组件都是使用python-memcached模块操作memcache的, 该模块天然支持集群操作, 但并没有做ha, 当memcache重启后数据丢失, 理论上这类数据丢失不会影响最终结果  

当前nova配置:
```
memcached_servers = scrumr1-controller-2:11211,scrumr1-controller-3:11211,scrumr1-controller-4:11211
```

#### 2. 存储keystone token
**目前平台是使用mysql后端存储的token, 无memcache单点问题, 不过可以考虑采用memcache_pool后端**

#### 3. 存储keystonemiddleware token


在系统访问api时，都要经过auth_token认证，只有认证成功才能继续访问api,所以弄清api认证的流程很有必要。

token认证包括了三个认证过程，即：**cache认证，本地认证和远程认证**；

1. 根据token信息从token cache获取token id 和具体的token信息(json字符串,cached)；返回cached；

2. 如果token cache中没有，则使用cms对token字符串进行解析认证；返回解析后的token信息,然后将计算是否超时；最后将没超时的token信息保存在 token cache中；返回解析后的字符串；

3. 如果cms解析失败，则进行远程token认证，即访问keystone server进行token 认证；

4. 如果远程认证成功，将计算是否超时；最后将没超时的token信息保存在 token cache中；返回解析后的字符串；

5. 如果远程认证失败，则抛出认证失败异常；

**从各个组件的配置上看,目前平台没有配置cache认证, 是一个优化点**

nova/nova.conf
```
[keystone_authtoken]
identity_uri = http://172.16.92.30:35357
auth_uri = http://172.16.92.30:5000/v2.0
admin_user=nova
admin_password = aaaaaa
admin_tenant_name = service
```


#### 4. 存储业务层用户session数据
目前业务层Django local_setting中配置的缓存后端是  

```
CACHES = {
     'default': {
         'BACKEND': 'django.core.cache.backends.memcached.PyLibMCCache',
         'LOCATION': '172.16.92.30:11211',
     }
}
```
该后端基于pylibmc客户端, 阅读文档后发现该客户端不支持多线程且pylibmc是建立在libmemcached的基础之上的, 如果memcached发生故障, libmemcached将报告一个错误, 并不会自动寻找下一节点, 因此该客户端本身并不支持故障转移状态机制

在架构上配置中的memcache地址是控制节点的VIP, 但memcache服务未通过haproxy配置负载均衡, 也就是说如果VIP所在控制节点的memcached服务down了则无法访问memcache, 导致平台无法登录, memcache存在单点问题


业务层memcache负载均衡方案
--------------------------
memcached虽然称为“分布式”缓存服务器，但服务器端并没有“分布式”功能。服务器端仅包括内存存储功能，其实现非常简单。至于memcached的分布式，则是完全由客户端程序库实现的。这种分布式是memcached的最大特点。


#### 方案1: django缓存后端使用 MemcachedCache

修改缓存后端为MemcachedCache, 该后端基于python-memcached客户端, openstack缓存操作也是基于这个模块, 并且python-memcached是线程安全的
	
```
CACHES = {
    'default': {
        'BACKEND': 'django.core.cache.backends.memcached.MemcachedCache',
        'LOCATION': [
            '172.19.26.240:11211',
            '172.19.26.242:11212',
            '172.19.26.244:11213',
        ]
    }
}
```
python-emcached客户端有一个非常好的特点就是可以让多个服务的缓存共享. 这就意味着你可以在多台机器上运行Memcached服务，这些程序将会把这几个机器当做同一个缓存，从而不需要复制每个缓存的值在每个机器上。为了使用这个特性，把所有的服务地址放在LOCATION里面，用分号隔开或者当做一个list, 同一地址配置多次会增加权重

python-memcached模块支持集群操作的原理是在内存中维护一个主机列表，且集群中主机的权重值和主机在列表中重复出现的次数成正比。  
在操作缓存时根据算法将key计算成一个数字, 再将数字和主机列表长度求余数, 得到目标主机, 这种方式的缺点是在增加节点的时候，余数就会产生巨变，这样就无法获取与保存时相同的服务器，从而影响缓存的命中率。 但是在咱们平台目前来看, 这也不是什么问题, 如果觉得不满意, 可以看看下面这种后端

修改缓存后端为MemcachedHashRingCache, 该后端基于python-memcached客户端, 不同的是这个后端是基于一致性哈系算法
```
CACHES = {
    'default': {
        'BACKEND': 'memcached_hashring.backend.MemcachedHashRingCache',
        'LOCATION': [
           '10.0.0.1:11211',
           '10.0.0.2:11211',
           '10.0.0.3:11211',
        ],
    },
}
```

这是一个集群负载均衡方案, 如果某个节点的服务挂掉, 会导致连接到该节点的用户登出


#### 方案2: 使用magent代理

![@1|center](../images/magent_proxy.jpg)

上图模型已经能够很好的解决多节点的数据备份和负载均衡, 但在节点故障恢复后, 不会从备份节点同步数据, 存在空值问题

解决办法大体的思路是用magent来实现负载均衡，利用repcached来实现单点恢复。  

使用magent+repcache的方式，可以最大程度利用服务器来存储不同的数据，和使用相同的资源；  
同时解决无法同步数据，容易造成单点故障等问题。

但是以我们目前的使用场景来看, memcache似乎没有必要做ha, 不仅增加架构复杂度, 并且浪费内存
方案1虽然存在单点恢复问题, 但最坏的结果就是节点故障导致该节点用户登出, 但不会导致整体服务不可用, 因此推荐方案1



